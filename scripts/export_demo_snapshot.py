#!/usr/bin/env python3
"""Export a spatially-cropped weather snapshot for the Windmar demo.

Connects to the local windmar-db container, reads all complete forecast
runs, crops grid data to the demo bounding box (30-60N, 30W-40E), and
writes a compressed SQL dump for the demo database.

Usage:
    python scripts/export_demo_snapshot.py

Output:
    data/demo-snapshot.sql.gz
"""

import gzip
import subprocess
import sys
import zlib
from io import BytesIO
from pathlib import Path

import numpy as np

# Demo bounding box (matches frontend/lib/demoMode.ts)
LAT_MIN, LAT_MAX = 30.0, 60.0
LON_MIN, LON_MAX = -30.0, 40.0

# 3 days of forecast = hours 0..72 (3-hourly)
MAX_FORECAST_HOUR = 72

CONTAINER = "windmar-db"
DB_USER = "windmar"
DB_NAME = "windmar"
OUTPUT = Path(__file__).resolve().parent.parent / "data" / "demo-snapshot.sql.gz"


def psql(query: str) -> str:
    """Run a query via docker exec and return stdout."""
    result = subprocess.run(
        ["docker", "exec", CONTAINER, "psql", "-U", DB_USER, "-d", DB_NAME,
         "-t", "-A", "-F", "\t", "-c", query],
        capture_output=True, text=True, check=True,
    )
    return result.stdout.strip()


def psql_bytes(query: str) -> bytes:
    """Run a query returning raw bytes via COPY."""
    result = subprocess.run(
        ["docker", "exec", CONTAINER, "psql", "-U", DB_USER, "-d", DB_NAME,
         "-t", "-A", "-c", query],
        capture_output=True, check=True,
    )
    return result.stdout


def decompress_array(hex_str: str, dtype=np.float32) -> np.ndarray:
    """Decompress a hex-encoded zlib-compressed array."""
    # Strip \\x prefix if present
    if hex_str.startswith("\\x"):
        hex_str = hex_str[2:]
    raw = bytes.fromhex(hex_str)
    decompressed = zlib.decompress(raw)
    return np.frombuffer(decompressed, dtype=dtype)


def compress_array(arr: np.ndarray) -> bytes:
    """Compress a numpy array with zlib."""
    return zlib.compress(arr.astype(np.float32).tobytes(), level=6)


def bytea_hex(data: bytes) -> str:
    """Format bytes as PostgreSQL hex bytea literal."""
    return "'\\x" + data.hex() + "'"


def escape_str(s) -> str:
    if s is None:
        return "NULL"
    return "'" + str(s).replace("'", "''") + "'"


def main():
    print("Connecting to windmar-db...")

    # Verify connectivity
    version = psql("SELECT version();")
    print(f"DB: {version[:60]}...")

    # Get complete forecast runs
    runs_raw = psql("""
        SELECT id, source, run_time, status, grid_resolution,
               lat_min, lat_max, lon_min, lon_max,
               forecast_hours::text, metadata::text, ingested_at
        FROM weather_forecast_runs
        WHERE status = 'complete'
        ORDER BY source, run_time DESC
    """)

    if not runs_raw:
        print("ERROR: No complete forecast runs found!")
        sys.exit(1)

    runs = [line.split("\t") for line in runs_raw.split("\n") if line.strip()]
    print(f"Found {len(runs)} complete forecast runs")

    # Open output
    OUTPUT.parent.mkdir(parents=True, exist_ok=True)
    gz = gzip.open(OUTPUT, "wt", compresslevel=6)

    gz.write("-- Windmar demo snapshot: weather forecast data\n")
    gz.write(f"-- Demo bounds: {LAT_MIN}-{LAT_MAX}N, {LON_MIN}-{LON_MAX}E\n")
    gz.write("-- Generated by export_demo_snapshot.py\n\n")
    gz.write("BEGIN;\n\n")

    # Clean existing weather data
    gz.write("TRUNCATE weather_grid_data CASCADE;\n")
    gz.write("TRUNCATE weather_forecast_runs CASCADE;\n\n")

    total_grids = 0

    for run in runs:
        run_id = run[0]
        source = run[1]
        run_time = run[2]
        status = run[3]
        grid_res = run[4] if run[4] else "NULL"
        lat_min = float(run[5]) if run[5] else LAT_MIN
        lat_max = float(run[6]) if run[6] else LAT_MAX
        lon_min = float(run[7]) if run[7] else LON_MIN
        lon_max = float(run[8]) if run[8] else LON_MAX
        forecast_hours_raw = run[9]  # Text like {0,3,6,...,120}
        # Clamp to 3 days
        fh_list = [int(h) for h in forecast_hours_raw.strip("{}").split(",") if h]
        fh_list = [h for h in fh_list if h <= MAX_FORECAST_HOUR]
        forecast_hours = "{" + ",".join(str(h) for h in fh_list) + "}"
        metadata = run[10] if run[10] else None
        ingested_at = run[11]

        # Clamp bounds to demo area
        c_lat_min = max(lat_min, LAT_MIN)
        c_lat_max = min(lat_max, LAT_MAX)
        c_lon_min = max(lon_min, LON_MIN)
        c_lon_max = min(lon_max, LON_MAX)

        gz.write(
            f"INSERT INTO weather_forecast_runs "
            f"(id, source, run_time, status, grid_resolution, "
            f"lat_min, lat_max, lon_min, lon_max, forecast_hours, metadata, ingested_at) "
            f"VALUES ({run_id}, {escape_str(source)}, {escape_str(run_time)}, "
            f"{escape_str(status)}, "
            f"{'NULL' if grid_res == 'NULL' else escape_str(grid_res)}, "
            f"{c_lat_min}, {c_lat_max}, {c_lon_min}, {c_lon_max}, "
            f"'{forecast_hours}', "
            f"{'NULL' if metadata is None else escape_str(metadata)}, "
            f"{escape_str(ingested_at)}"
            f") ON CONFLICT DO NOTHING;\n"
        )

        # Get grid data as hex-encoded bytea (capped to 3 days)
        grids_raw = psql(f"""
            SELECT id, forecast_hour, parameter,
                   encode(lats, 'hex'), encode(lons, 'hex'),
                   encode(data, 'hex'), shape_rows, shape_cols
            FROM weather_grid_data
            WHERE run_id = {run_id}
              AND forecast_hour <= {MAX_FORECAST_HOUR}
            ORDER BY forecast_hour, parameter
        """)

        if not grids_raw:
            print(f"  {source}: 0 grids (empty)")
            continue

        grids = [line.split("\t") for line in grids_raw.split("\n") if line.strip()]
        cropped_count = 0

        for grid in grids:
            gid, fhour, param = grid[0], grid[1], grid[2]
            lats_hex, lons_hex, data_hex = grid[3], grid[4], grid[5]
            srows, scols = int(grid[6]), int(grid[7])

            # Decompress
            lats = decompress_array(lats_hex)
            lons = decompress_array(lons_hex)
            data = decompress_array(data_hex).reshape(srows, scols)

            # Find indices within demo bounds
            lat_mask = (lats >= LAT_MIN) & (lats <= LAT_MAX)
            lon_mask = (lons >= LON_MIN) & (lons <= LON_MAX)

            if not lat_mask.any() or not lon_mask.any():
                continue

            lat_idx = np.where(lat_mask)[0]
            lon_idx = np.where(lon_mask)[0]

            cropped_lats = compress_array(lats[lat_idx])
            cropped_lons = compress_array(lons[lon_idx])
            cropped_data = compress_array(data[np.ix_(lat_idx, lon_idx)])
            new_rows = len(lat_idx)
            new_cols = len(lon_idx)

            gz.write(
                f"INSERT INTO weather_grid_data "
                f"(id, run_id, forecast_hour, parameter, lats, lons, data, "
                f"shape_rows, shape_cols) VALUES ("
                f"{gid}, {run_id}, {fhour}, {escape_str(param)}, "
                f"{bytea_hex(cropped_lats)}, {bytea_hex(cropped_lons)}, "
                f"{bytea_hex(cropped_data)}, "
                f"{new_rows}, {new_cols}"
                f") ON CONFLICT DO NOTHING;\n"
            )
            cropped_count += 1

        total_grids += cropped_count
        print(f"  {source}: {cropped_count}/{len(grids)} grids cropped")

    # Reset sequences
    gz.write("\nSELECT setval('weather_forecast_runs_id_seq', "
             "(SELECT COALESCE(MAX(id), 0) FROM weather_forecast_runs));\n")
    gz.write("SELECT setval('weather_grid_data_id_seq', "
             "(SELECT COALESCE(MAX(id), 0) FROM weather_grid_data));\n")

    gz.write("\nCOMMIT;\n")
    gz.close()

    file_size = OUTPUT.stat().st_size / (1024 * 1024)
    print(f"\nDone! {total_grids} grids exported")
    print(f"Output: {OUTPUT} ({file_size:.1f} MB)")


if __name__ == "__main__":
    main()
